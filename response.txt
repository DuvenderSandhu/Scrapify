async def get_html(url: str, button: str = None, options: dict = None, loader: str = None) -> str:
    """
    Fetch HTML content by navigating to a URL and extracting a strictly meaningful container.
    """
    ua = UserAgent(os=random.choice(ua_os),platforms=random.choice(ua_platform))
    print("ua",ua.random)
    options = options or {}
    max_pages = options.get('max_pages', 1)
    handle_lazy_loading = options.get('handle_lazy_loading', False)
    numbered = options.get('pagination_method', False) == "Numbered"
    handle_pagination = options.get('handle_pagination', False)
    js_timeout = options.get('js_timeout', 10000)           # Reduced JS timeout (ms)
    navigation_timeout = options.get('navigation_timeout', 30000)  # Reduced navigation timeout (ms)
    retry_attempts = options.get('retry_attempts', 2)
    
    start_time = time.time()
    log_info(f"Starting HTML fetch from: {url} [Options: lazy_loading={handle_lazy_loading}, pagination={handle_pagination}]")

    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=True,
            proxy={
                "server": "http://tl-c8c9ce2df202f54d955865e4ebb8ff2c31713b0eea7d6b034312a7708c5f008e-country-IN:74s2i9dkssdh@proxy.toolip.io:31114"
            },
            args=['--disable-web-security', '--disable-features=IsolateOrigins,site-per-process']
        )
        log_info("Launched headless Chromium browser with args: --disable-web-security")
        
        for attempt in range(retry_attempts + 1):
            if attempt > 0:
                log_info(f"Retry attempt {attempt + 1}/{retry_attempts + 1}")
            
            print(selected_user_agent)
            # Create a fresh context and page for each attempt.
            context = await browser.new_context(
                viewport={"width": 1920, "height": 1080},
                user_agent=ua.random,
                ignore_https_errors=True,
                java_script_enabled=True
            )
            page = await context.new_page()
            print(http_headers)
            # await page.set_extra_http_headers(http_headers)
            page.set_default_timeout(navigation_timeout)
            await random_sleep()
            # page.on("dialog", lambda dialog: asyncio.create_task(dialog.dismiss()))
            start_x, start_y = 100, 100
            await page.mouse.move(start_x, start_y)
            # End position (e.g., somewhere in the middle of the page)
            end_x, end_y = 600, 600

            # Perform the human-like zigzag mouse movement
            await random_zigzag_move(page, start_x, start_y, end_x, end_y)
            try:
                log_info(f"Navigating to {url} with timeout {navigation_timeout}ms")
                nav_start = time.time()
                response = await page.goto(url, timeout=navigation_timeout, wait_until="domcontentloaded")
                nav_time = time.time() - nav_start
                
                if response and response.ok:
                    log_success(f"Navigation succeeded in {nav_time:.2f}s (Status: {response.status})")
                else:
                    log_warning(f"Navigation issue after {nav_time:.2f}s: Status={response.status if response else 'No response'}")
                    await context.close()
                    continue

                # Wait for the initial network activities to settle.
                await page.wait_for_timeout(3000)
                try:
                    await page.wait_for_load_state("networkidle", timeout=js_timeout // 2)
                    log_success("Page reached network idle state")
                except Exception as e:
                    # log_warning(f"Network idle timeout: {str(e)}")
                    print(str(e))
                
                if handle_pagination and button:
                    await handle_pagination_with_backoff(page, button, loader, max_pages)
                if handle_pagination and numbered:
                    await handle_numbered_pagination_with_backoff(page, url)
                if handle_lazy_loading:
                    await handle_lazy_loading_with_limits(page)
                
                await page.wait_for_timeout(2000)
                html_content = await page.content()
                raw_size = len(html_content)
                
                if raw_size > 500:
                    log_success(f"Extracted raw HTML: {raw_size} bytes")
                    filtered_html = html_content #extract_relevant_container(html_content)
                    filtered_size = len(filtered_html)
                    reduction = ((raw_size - filtered_size) / raw_size * 100)
                    log_info(f"Filtered content size: {filtered_size} bytes (reduction: {reduction:.1f}%)")
                    global rawid
                    if options.get('saveToDb', False):
                        print("Saving Raw")
                        rawid =  db.save_raw_html(url, filtered_html)
                    await context.close()
                    log_info(f"Fetch completed in {time.time() - start_time:.2f}s")
                    await browser.close()
                    return filtered_html
                else:
                    log_warning(f"Raw HTML too small ({raw_size} bytes), retrying")
                    await context.close()
                    continue
                    
            except Exception as e:
                log_error(f"Error during fetch: {str(e)}")
                await context.close()
            # End of attempt loop
        
        await browser.close()
        log_info("Browser closed")
        log_error(f"All {retry_attempts + 1} attempts failed for {url}")
        return ""
